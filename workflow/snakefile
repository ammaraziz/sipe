import os
import re
import time
import pandas as pd
from pathlib import Path
from shutil import copyfile
from datetime import datetime

configfile: "config/config.json"
reference = Path(config["reference"])
bed_file = Path(config["bed_file"])
out_dir = Path(config["out_dir"])
input_dir = Path(config["input_dir"])
#samples_path = Path(config["samples_path"])

# get sample names 
#samples = pd.read_table(samples_path, sep="\t")
SAMPLE_NAME, SAMPLE_NUMBER, PAIR = glob_wildcards(input_dir / "{sample_name}_{sample_number}_L001_{pair}_001.fastq.gz")
SAMPLES = list(set([i + "_" + x for i, x in zip(SAMPLE_NAME, SAMPLE_NUMBER)]))

rule all:
    input:
        expand(out_dir / "qc/{sample}.html", sample = SAMPLES),
        expand(out_dir / "align/{sample}/{sample}.bam", sample = SAMPLES),
        expand(out_dir / "status/{sample}.mkblastdb.bam", sample = SAMPLES),
        expand(out_dir / "status/{sample}.split.bam", sample = SAMPLES),
        expand(out_dir / "depth/raw/{sample}.txt", sample = SAMPLES),
        #expand(out_dir / "depth/raw/{sample}.png", sample = SAMPLES),

rule qc:
	message: "Running fastp on {wildcards.sample}"
	input:
		forward = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"]),
		rev = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"]),
	output:
		report = out_dir / "qc/{sample}.html",
		stats = out_dir / "qc/{sample}.stats",
		forward = out_dir / "qc/{sample}_R1.fastq.gz",
		rev = out_dir / "qc/{sample}_R2.fastq.gz",
	params:
		primers = "../resources/primers.fasta",
	conda: "envs/qc.yaml"
	shell:"""
	fastp \
	-i {input.forward} \
	-I {input.rev} \
	-o {output.forward} \
	-O {output.rev} \
	--adapter_fasta {params.primers} \
	-h {output.report} 2> {output.stats}
	"""

rule mkblastdb:
	message: "Creating blast db"
	input:
		reference = reference
	output:
		status = out_dir / "status/{sample}.mkblastdb"
	threads: 1
	conda: "envs/blast.yaml"
	shell:"""
	makeblastdb \
	-in {input.reference} \
	-title "entero" \
	-dbtype nucl \
	-hash_index
	"""

rule run_blast:
	message: "Classifying reads using blast"
	input:
		forward = out_dir / "qc/{sample}_R1.fastq.gz"
	output:
		results = out_dir / "blast/{sample}.result.xml"
	params:
		db = reference.parent / reference.stem
	threads: 8
	shell:"""
	seqkit fq2fa {input.forward} \
	| blastn -outfmt 5 -task megablast \
	-out {output.results} \
	-db {params.db} \
	-max_target_seqs 1P \
	-num_threads {threads} 2> dev/null
	"""

rule parse_blast:
	message: "Filtering blast results for reads > 500 between genome region: 750 - 3500"
	input:
		xml = rules.run_blast.output.result
	output:
		tsv = out_dir / "blast/{sample}.sorted.tsv"
	params:
		out_dir = out_dir
	run:
		from Bio.Blast import NCBIXML
		from collections import defaultdict
		from csv import DictWriter
		
		result_handle = open(input.xml)
		blast_records = NCBIXML.parse(result_handle)
		
		reads_by_entero = defaultdict(list)
		blast_results = []

		for record in blast_records:
			read_name = record.query
			top_hit = record.alignments[0].hit_def
			
			query_start = record.alignments[0].hsps[0].query_start
			query_end = record.alignments[0].hsps[0].query_end
			subject_start = record.alignments[0].hsps[0].sbjct_start
			subject_end = record.alignments[0].hsps[0].sbjct_end
			score = record.alignments[0].hsps[0].score

			reads_by_entero[top_hit].append(read_name)
			
			if subject_start > 750 and subject_end < 3500:
				blast_results.append(
						{
						'read_name' : read_name,
						'top_hit' : top_hit,
						'query_start' : query_start,
						'query_end' : query_end,
						'subject_start' : subject_start,
						'subject_end' : subject_end,
						'score' : score
						}
						)

		# write out all results
		keys = ['read_name', 'top_hit', 'query_start', 'query_end', 'subject_start', 'subject_end', 'score']
		with open(output.tsv, 'wb') as tsv:
			dict_writer = csv.DictWriter(tsv, keys, delimiter = "\t")
			dict_writer.writeheader()
			dict_writer.writerows(blast_results)
		# write out read names into individual files
		# only hits with > 500 reads
		for org in reads_by_entero:
			if len(org) > 500
				with open(params.out_dir / "blast" / (f"{sample}_{org}_reads.txt"), 'w') as file:
					for item in org:
						file.write(f'{item}\n')

def aggregate_input(wildcards):
    checkpoints.parse_blast.get(sample=wildcards.sample)
    orgs = glob_wildcards(f"{wildcards.out_dir}/blast/{wildcards.sample}_{{org}}_reads.txt").orgs
    return expand(f"{wildcards.out_dir}/blast/{wildcards.sample}_{{org}}_reads.txt", id=orgs)

rule split_blast_reads:
	input:
		orgs = glob_wildcards(out_dir / "{org}_reads.txt")
		forward = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"]),
		rev = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"])
	output:
		status = out_dir / "status/{sample}.split.txt"
	threads: 8
	shell:"""
	seqkit grep -f 
	"""

rule build_index:
	message: "Building bowtie2 index"
	input:
		reference = reference
	output:
		status = out_dir / "status/bowtie2.index"
	params:
		prefix = reference.parent / reference.stem
	threads: 1
	conda: "envs/align.yaml"
	shell:"""
	bowtie2-build -q {input.reference} {params.prefix}
	touch {output}
	"""

rule align:
	message: "Aligning with bowtie2"
	input:
		forward = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"]),
		rev = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R2"]),
	output:
		bam = out_dir / "align/{sample}/{sample}.bam",
		stats = out_dir / "align/{sample}/{sample}.stats",
	params:
		index = reference,
	threads: 20
	conda: "envs/align.yaml"
	shell:"""
	bowtie2 -q \
	-p {threads} \
	-x {params.index} \
	-1 {input.forward} \
	-2 {input.rev} \
	--very-sensitive-local \
	2> {output.stats} \
	| samtools view -b - \
	| samtools sort -@ {threads} - 1> {output.bam} 2> /dev/null
	samtools index {output.bam}
	"""

rule split_bam:
	message: "Splitting BAM files by chromosome"
	input:
		bam = rules.align.output.bam
	output:
		status = out_dir / "status/{sample}.split.bam"
	params:
		split_prefix = out_dir / "align/{sample}/"
	shell:"""
	bamtools split -in {input.bam} -refPrefix {params.split_prefix} -reference
	touch {output.status}
	"""

rule depth:
	message: "Calculating depth per alignment"
	input:
		bam = rules.align.output.bam
	output:
		depth = out_dir / "depth/raw/{sample}.txt"
	shell:"""
	samtools depth -a {input.bam} -o {output.depth}
	"""

# rule plot_depth:
# 	message: "plotting depth"
# 	input:
# 		depth = rules.depth.output.plot
# 	output:
# 		png = outdir / "depth/raw/{sample}.png"
# 	run:
# 		import .scripts










