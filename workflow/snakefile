import os
import re
import time
import pandas as pd
from pathlib import Path
from shutil import copyfile
from datetime import datetime

configfile: "config/config.json"
reference_blast = Path(config["reference_blast"])
reference_wgf = Path(config["reference_wgf"])

bed_file = Path(config["bed_file"])
out_dir = Path(config["out_dir"])
input_dir = Path(config["input_dir"])

# get sample names 
SAMPLE_NAME, SAMPLE_NUMBER, PAIR = glob_wildcards(input_dir / "{sample_name}_{sample_number}_L001_{pair}_001.fastq.gz")
SAMPLES = list(set([i + "_" + x for i, x in zip(SAMPLE_NAME, SAMPLE_NUMBER)]))

rule all:
	input:
		expand(out_dir / "qc/{sample}.html", sample = SAMPLES),
		expand(out_dir / "blast/{sample}.result.xml", sample = SAMPLES),
		expand(out_dir / "status/{sample}.parseblast", sample = SAMPLES),
		expand(out_dir / "status/{sample}.reference", sample = SAMPLES),
		expand(out_dir / "status/{sample}.bowtie2.index", sample = SAMPLES),
		expand(out_dir / "status/{sample}.align", sample = SAMPLES),
		expand(out_dir / "status/{sample}.bamsplit", sample = SAMPLES),
		expand(out_dir / "status/{sample}.consensus", sample = SAMPLES),
		expand(out_dir / "status/{sample}.depth", sample = SAMPLES),
		out_dir / "status/plotting_complete.txt"

rule qc:
	message: "Running fastp on {wildcards.sample}"
	input:
		forward = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"]),
		rev = expand(input_dir / "{{sample}}_L001_{pair}_001.fastq.gz", pair = ["R1"]),
	output:
		report = out_dir / "qc/{sample}.html",
		stats = out_dir / "qc/{sample}.stats",
		forward = out_dir / "qc/{sample}_R1.fastq.gz",
		rev = out_dir / "qc/{sample}_R2.fastq.gz",
	params:
		primers = "resources/primers.fasta",
	conda: "envs/qc.yaml"
	shell:"""
	fastp \
	-i {input.forward} \
	-I {input.rev} \
	-o {output.forward} \
	-O {output.rev} \
	--adapter_fasta {params.primers} \
	--json /dev/null \
	-h {output.report} 2> {output.stats}
	"""

rule run_blast:
	message: "Classifying reads using blast"
	input:
		forward = out_dir / "qc/{sample}_R1.fastq.gz"
	output:
		results = out_dir / "blast/{sample}.result.xml"
	params:
		db = reference_blast,
		blast_dir = out_dir / "blast"
	conda: "envs/blast.yaml"
	threads: 4
	shell:"""
	mkdir -p {params.blast_dir}

	seqkit fq2fa {input.forward} \
	| blastn -outfmt 5 -task megablast \
	-out {output.results} \
	-db {params.db} \
	-max_target_seqs 1 \
	-num_threads 8 2> /dev/null
	"""

checkpoint parse_blast:
	message: "Filtering blast results for reads > 500 between genome region: 750 - 3500"
	input:
		xml = rules.run_blast.output.results
	output:
		status = out_dir / "status/{sample}.parseblast",
	params:
		out_dir = out_dir / "blast/"
	threads: 1
	conda: "envs/blast.yaml"
	shell:"""
	python scripts/parse_blast.py \
	--input-xml {input.xml} \
	--output-dir {params.out_dir} \
	--sample {wildcards.sample}

	touch {output.status}
	"""

def aggregate_blast(wildcards):
	
	# create directory for serotype data
	import os
	if not os.path.isdir(f"{out_dir}/align/"):
		os.makedirs(f"{out_dir}/align/")

	serotypes = glob_wildcards(f"{out_dir}/blast/{wildcards.sample}_{{serotype}}_reads.txt").serotype

	with open(f"{out_dir}/align/{wildcards.sample}.txt", 'w') as f:
		for sero in serotypes:
			f.write(f"{sero}\n")
	output = f"{out_dir}/align/{wildcards.sample}.txt"
	checkpoints.parse_blast.get(sample=wildcards.sample)
	return(output)

rule create_reference:
	message: "Creating sample specific reference"
	input:
		seros = aggregate_blast,
	output:
		status = out_dir / "status/{sample}.reference",
		reference = out_dir / "align/{sample}_reference.fasta",
	params:
		reference = reference_wgf,
	conda: "envs/blast.yaml"
	shell:"""
	seqkit grep {params.reference} -r -f {input.seros} \
	| seqkit replace -p "(_.+$)" -r "" -o {output.reference}
	touch {output.status}
	"""

rule build_index:
	message: "Building bowtie2 index"
	input:
		reference = rules.create_reference.output.reference
	output:
		status = out_dir / "status/{sample}.bowtie2.index"
	params:
		prefix = "{sample}",
		out_dir = out_dir / "align/"
	threads: 1
	conda: "envs/align.yaml"
	shell:"""
	bowtie2-build -q {input.reference} {params.out_dir}/{params.prefix}
	touch {output}
	"""

rule align:
	message: "Aligning reads with bowtie2 to custom reference"
	input:
		forward = rules.qc.output.forward,
		rev = rules.qc.output.rev,
		index_status = out_dir / "status/{sample}.bowtie2.index"
	output:
		bam = out_dir / "align/{sample}/{sample}.bam",
		stats = out_dir / "align/{sample}/{sample}.stats",
		status = out_dir / "status/{sample}.align",
	params:
		index = "{sample}",
		out_dir = out_dir / "align/",
	threads: 5
	conda: "envs/align.yaml"
	shell:"""
	bowtie2 -q \
	-p {threads} \
	-x {params.out_dir}/{params.index} \
	-1 {input.forward} \
	-2 {input.rev} \
	--very-sensitive-local \
	2> {output.stats} \
	| samtools view -b - \
	| samtools sort -@ {threads} - 1> {output.bam} 2> /dev/null
	samtools index {output.bam}

	cat {input.index_status} > {output.status}
	"""

checkpoint split_bam:
	message: "Splitting Bam files"
	input:
		bam = rules.align.output.bam
	output:
		status = out_dir / "status/{sample}.bamsplit"
	conda: "envs/align.yaml"
	threads: 1
	shell:"""
	bamtools split -in {input.bam} -reference 
	touch {output.status}
	"""

def aggregate_bam(wildcards):
	import glob
	import os
	bams = [
	os.path.basename(x) for x 
	in glob.glob(f"{out_dir}/align/{wildcards.sample}/*REF*.bam") 
	if "REF_unmapped.bam" not in x
	]
	output = expand(f"{out_dir}/align/{wildcards.sample}/{{bam}}", bam = bams)
	checkpoints.split_bam.get(sample=wildcards.sample)
	return(output)


rule consensus:
	message: "Generating consensus for {wildcards.sample}"
	input:
		bams = aggregate_bam
	output:
		status = out_dir / "status/{sample}.consensus"
	params:
		input_dir = out_dir / "align/{sample}/",
		out_dir = out_dir / "consensus/",
	conda: "envs/consensus.yaml"
	log: out_dir / "consensus/{sample}.log"
	shell:"""
	mkdir -p {params.out_dir}

	for bam in {input.bams}; do
		f=$(basename $bam)
		samtools mpileup -a -d 8000 -A -Q 0 $bam \
		| ivar consensus -t 0.75 -m 10 -n N -p {params.out_dir}/$f >> {log}
	done
	touch {output.status}
	"""

rule depth:
	message: "Creating depth from bam file and plotting"
	input:
		bams = aggregate_bam
	output:
		status = out_dir / "status/{sample}.depth"
	params:
		out_dir = out_dir / "depth/",
		input_dir = out_dir / "align/{sample}/",
	conda: "envs/consensus.yaml"
	shell:"""
	mkdir -p {params.out_dir}
	for bam in {input.bams}; do
		f=$(basename $bam)
		samtools depth -a $bam > {params.out_dir}/$f.depth.txt
	done
	touch {output.status}
	"""

rule plot_depth:
	message: "plotting depth"
	input:
		expand(out_dir / "status/{sample}.depth", sample = SAMPLES)
	output:
		status = out_dir / "status/plotting_complete.txt"
	params:
		depth_dir = out_dir / "depth/"
	conda: "envs/plot.yaml"
	shell:"""
	for f in {depth_dir}*.txt;
		bname=${basename $f}
		
		python scripts/depth_plot.py \
		--input $f \
		--output {params.output}/${bname/.txt/.pdf}\
	done
	touch
	"""





